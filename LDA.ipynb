{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from numpy import linalg as LA\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.decomposition import NMF,LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of text documents for 1 author\n",
    "text = []\n",
    "authors = 10\n",
    "#for i in range(authors):\n",
    "for j in range(16,21):\n",
    "    filename = \"articles/articles/test/author_\"+str(authors)+\"/\"+str(j)+\".txt\"\n",
    "    with open(filename, 'r' ,encoding = \"utf8\") as file:\n",
    "        sentence = file.read()\n",
    "        sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "        text.append(sentence.lower())                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert nltk tag to wordnet tag\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_lemmatizer(sentence):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return lemmatized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NMF is able to use tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(text)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(text)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sailalitha\\Anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:314: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "no_topics = 3\n",
    "\n",
    "# Run NMF\n",
    "#nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "\n",
    "# Run LDA\n",
    "lda = LatentDirichletAllocation(n_topics=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.404514\n",
      "1    0.213036\n",
      "2    0.382450\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(lda.transform(tf))\n",
    "print(np.mean(df,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "got tourney cool home nt today game cities hey fun\n",
      "Topic 1:\n",
      "school summer game did watched love got nt home hung\n",
      "Topic 2:\n",
      "just games cities summer weekend tourney busy hey later love\n"
     ]
    }
   ],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print (\"Topic %d:\" % (topic_idx))\n",
    "        print (\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "no_top_words = 10\n",
    "#display_topics(nmf, tfidf_feature_names, no_top_words)\n",
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Sailalitha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Sailalitha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['hey', 'my', 'weekend', 'be', 'pretty', 'good', 'my', 'mom', 'and', 'i', 'drive', 'down', 'on', 'friday', 'night', 'and', 'it', 'be', 'stormy', 'and', 'we', 'get', 'there', 'and', 'i', 'find', 'a', 'lot', 'of', 'girl', 'on', 'my', 'soccer', 'team', 'we', 'all', 'get', 'in', 'our', 'bikini', 'and', 'hung', 'out', 'at', 'the', 'pool', 'and', 'meet', 'some', 'guy', 'that', 'be', 'at', 'a', 'state', 'track', 'meet', 'they', 'be', 'really', 'hot', 'with', 'ripped', 'stomach', 'muscle', 'and', 'prolly', '16', 'or', '17', 'we', 'saw', 'them', 'again', 'later', 'and', 'everyone', 'be', 'play', 'it', 'all', 'cool', 'as', 'we', 'flirt', 'with', 'them', 'and', 'i', 'have', 'to', 'be', 'dumb', 'and', 'blurt', 'out', 'why', 'be', 'your', 'shirt', 'on', 'i', 'm', 'just', 'too', 'cool', 'on', 'saturday', 'we', 'lose', 'two', 'game', 'which', 'suck', 'cuz', 'the', 'team', 'we', 'palyed', 'be', 'nt', 'good', 'but', 'we', 'be', 'miss', 'people', 'and', 'die', 'from', 'the', 'heat', 'lol', 'anyways', 'then', 'my', 'mom', 'take', 'me', 'shopping', 'and', 'i', 'get', 'lipgloss', 'and', 'perfume', 'from', 'bath', 'and', 'body', 'work', '5', 'thong', 'victoria', 's', 'secret', 'a', 'pair', 'of', 'capri', 'and', 'a', 'belt', 'from', 'abercrombie', 'a', 'buttonup', 'white', 'strip', 'shirt', 'from', 'american', 'eagle', 'and', 'two', 'book', 'wicked', 'and', 'fall', 'angel', 'then', 'that', 'night', 'we', 'all', 'meet', 'some', 'guy', 'our', 'age', 'who', 'have', 'a', 'bball', 'tourney', 'they', 'be', 'from', 'lacrosse', 'wisconsin', 'and', 'they', 'be', 'nice', 'we', 'get', 'there', 'screennames', 'and', 'breanna', 'amanda', 'emily', 'g', 'christine', 'and', 'i', 'go', 'to', 'their', 'room', 'and', 'watch', '50', 'first', 'date', 'with', 'them', 'it', 'be', 'fun', 'then', 'today', 'we', 'check', 'out', 'of', 'the', 'hotel', 'and', 'my', 'mom', 'and', 'i', 'do', 'a', 'little', 'more', 'shopping', 'and', 'i', 'get', 'a', 'tank', 'top', 'then', 'my', 'mom', 'and', 'i', 'go', 'to', 'the', 'movie', 'save', 'it', 'be', 'funny', 'about', 'a', 'school', 'of', 'jesusfreaks', 'and', 'the', 'main', 'character', 'jena', 'malone', 'get', 'pregnant', 'and', 'mandy', 'moore', 'make', 'her', 'life', 'hell', 'then', 'we', 'have', 'a', 'soccer', 'game', 'and', 'i', 'play', 'forward', 'and', 'i', 'score', 'the', 'first', 'goal', 'of', 'the', 'tourney', 'and', 'then', 'make', 'an', 'assist', 'with', 'kayley', 'malban', 'for', 'the', 'secondlast', 'goal', 'win', 'that', 'game', 'get', 'us', 'third', 'place', 'in', 'the', 'tourney', 'and', 'we', 'get', 'these', 'big', 'gaudy', 'medal', 'then', 'my', 'mom', 'and', 'i', 'drive', 'home', 'and', 'here', 'i', 'be', 'love', 'keely'], ['hey', 'i', 'm', 'go', 'down', 'to', 'the', 'city', 'today', 'for', 'a', 'soccer', 'tourney', 'all', 'weekend', 'i', 'm', 'so', 'busy', 'with', 'soccer', 'all', 'of', 'the', 'time', 'it', 's', 'crazy', 'i', 'have', 'nt', 'hang', 'out', 'with', 'any', 'of', 'my', 'friend', 'other', 'then', 'my', 'soccer', 'friend', 'much', 'but', 'i', 'do', 'hang', 'out', 'with', 'austin', 'and', 'my', 'neighborfriend', 'joe', 'serre', 'he', 's', 'a', 'cool', 'kid', 'and', 'it', 's', 'fun', 'to', 'have', 'a', 'friend', 'close', 'by', 'he', 's', 'a', 'cutie', 'too', 'well', 'i', 'get', 'ta', 'go', 'more', 'when', 'i', 'get', 'home', 'from', 'the', 'city', 'keely'], ['hey', 'i', 've', 'be', 'sooooo', 'busy', 'with', 'soccer', 'the', 'past', 'week', 'that', 'school', 'have', 'be', 'out', 'it', 's', 'not', 'even', 'funny', 'we', 've', 'have', 'a', 'couple', 'game', 'in', 'the', 'city', 'some', 'home', 'game', 'practice', 'just', 'about', 'every', 'day', 'and', 'a', 'tourney', 'in', 'the', 'city', 'this', 'weekend', 'too', 'well', 'i', 'g2g', 'more', 'late', 'love', 'keely'], ['mmmm', 'i', 'm', 'love', 'summer', 'even', 'tho', 'i', 'm', 'not', 'do', 'a', 'whole', 'lot', 'it', 's', 'so', 'nice', 'to', 'sleep', 'in', 'and', 'have', 'sleepover', 'and', 'whatnot', 'all', 'of', 'my', 'sad', 'nostalgia', 'wear', 'off', 'and', 'i', 'm', 'just', 'happy', 'it', 's', 'summer', 'i', 'miss', 'a', 'soccer', 'game', 'on', 'accident', 'and', 'my', 'coach', 'be', 'gon', 'na', 'kill', 'me', 'i', 'm', 'scared', 'tongiht', 'i', 'think', 'i', 'm', 'sleep', 'at', 'my', 'sister', 's', 'house', 'and', 'get', 'that', 'nummy', 'cheesecake', 'yes', 'well', 'that', 's', 'enough', 'for', 'now', 'i', 'just', 'want', 'to', 'keep', 'you', 'update', 'keely'], ['school', 'be', 'over', 'and', 'summer', 'be', 'here', 'i', 'm', 'lovin', 'it', 'yesterday', 'i', 'do', 'nt', 'cry', 'at', 'school', 'but', 'i', 'do', 'when', 'i', 'get', 'home', 'think', 'about', 'how', 'middle', 'school', 'end', 'and', 'stuff', 'tonight', 'i', 'have', 'a', 'party', 'and', 'a', 'sleepover', 'and', 'tomorrow', 'i', 'have', 'a', 'soccer', 'game', 'yesterday', 'i', 'hang', 'out', 'with', 'nikita', 'and', 'watch', 'a', 'soccer', 'game', 'i', 'love', 'summer', 'keely']]\n"
     ]
    }
   ],
   "source": [
    "cleaned_text = [tokenizer_lemmatizer(sentence) for sentence in text]\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "import gensim.corpora as corpora\n",
    "id2word = corpora.Dictionary(cleaned_text)\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in cleaned_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('16', 1),\n",
       "  ('17', 1),\n",
       "  ('5', 1),\n",
       "  ('50', 1),\n",
       "  ('a', 10),\n",
       "  ('abercrombie', 1),\n",
       "  ('about', 1),\n",
       "  ('again', 1),\n",
       "  ('age', 1),\n",
       "  ('all', 3),\n",
       "  ('amanda', 1),\n",
       "  ('american', 1),\n",
       "  ('an', 1),\n",
       "  ('and', 33),\n",
       "  ('angel', 1),\n",
       "  ('anyways', 1),\n",
       "  ('as', 1),\n",
       "  ('assist', 1),\n",
       "  ('at', 2),\n",
       "  ('bath', 1),\n",
       "  ('bball', 1),\n",
       "  ('be', 14),\n",
       "  ('belt', 1),\n",
       "  ('big', 1),\n",
       "  ('bikini', 1),\n",
       "  ('blurt', 1),\n",
       "  ('body', 1),\n",
       "  ('book', 1),\n",
       "  ('breanna', 1),\n",
       "  ('but', 1),\n",
       "  ('buttonup', 1),\n",
       "  ('capri', 1),\n",
       "  ('character', 1),\n",
       "  ('check', 1),\n",
       "  ('christine', 1),\n",
       "  ('cool', 2),\n",
       "  ('cuz', 1),\n",
       "  ('date', 1),\n",
       "  ('die', 1),\n",
       "  ('do', 1),\n",
       "  ('down', 1),\n",
       "  ('drive', 2),\n",
       "  ('dumb', 1),\n",
       "  ('eagle', 1),\n",
       "  ('emily', 1),\n",
       "  ('everyone', 1),\n",
       "  ('fall', 1),\n",
       "  ('find', 1),\n",
       "  ('first', 2),\n",
       "  ('flirt', 1),\n",
       "  ('for', 1),\n",
       "  ('forward', 1),\n",
       "  ('friday', 1),\n",
       "  ('from', 5),\n",
       "  ('fun', 1),\n",
       "  ('funny', 1),\n",
       "  ('g', 1),\n",
       "  ('game', 3),\n",
       "  ('gaudy', 1),\n",
       "  ('get', 8),\n",
       "  ('girl', 1),\n",
       "  ('go', 2),\n",
       "  ('goal', 2),\n",
       "  ('good', 2),\n",
       "  ('guy', 2),\n",
       "  ('have', 3),\n",
       "  ('heat', 1),\n",
       "  ('hell', 1),\n",
       "  ('her', 1),\n",
       "  ('here', 1),\n",
       "  ('hey', 1),\n",
       "  ('home', 1),\n",
       "  ('hot', 1),\n",
       "  ('hotel', 1),\n",
       "  ('hung', 1),\n",
       "  ('i', 13),\n",
       "  ('in', 2),\n",
       "  ('it', 4),\n",
       "  ('jena', 1),\n",
       "  ('jesusfreaks', 1),\n",
       "  ('just', 1),\n",
       "  ('kayley', 1),\n",
       "  ('keely', 1),\n",
       "  ('lacrosse', 1),\n",
       "  ('later', 1),\n",
       "  ('life', 1),\n",
       "  ('lipgloss', 1),\n",
       "  ('little', 1),\n",
       "  ('lol', 1),\n",
       "  ('lose', 1),\n",
       "  ('lot', 1),\n",
       "  ('love', 1),\n",
       "  ('m', 1),\n",
       "  ('main', 1),\n",
       "  ('make', 2),\n",
       "  ('malban', 1),\n",
       "  ('malone', 1),\n",
       "  ('mandy', 1),\n",
       "  ('me', 1),\n",
       "  ('medal', 1),\n",
       "  ('meet', 3),\n",
       "  ('miss', 1),\n",
       "  ('mom', 5),\n",
       "  ('moore', 1),\n",
       "  ('more', 1),\n",
       "  ('movie', 1),\n",
       "  ('muscle', 1),\n",
       "  ('my', 7),\n",
       "  ('nice', 1),\n",
       "  ('night', 2),\n",
       "  ('nt', 1),\n",
       "  ('of', 5),\n",
       "  ('on', 4),\n",
       "  ('or', 1),\n",
       "  ('our', 2),\n",
       "  ('out', 3),\n",
       "  ('pair', 1),\n",
       "  ('palyed', 1),\n",
       "  ('people', 1),\n",
       "  ('perfume', 1),\n",
       "  ('place', 1),\n",
       "  ('play', 2),\n",
       "  ('pool', 1),\n",
       "  ('pregnant', 1),\n",
       "  ('pretty', 1),\n",
       "  ('prolly', 1),\n",
       "  ('really', 1),\n",
       "  ('ripped', 1),\n",
       "  ('room', 1),\n",
       "  ('s', 1),\n",
       "  ('saturday', 1),\n",
       "  ('save', 1),\n",
       "  ('saw', 1),\n",
       "  ('school', 1),\n",
       "  ('score', 1),\n",
       "  ('screennames', 1),\n",
       "  ('secondlast', 1),\n",
       "  ('secret', 1),\n",
       "  ('shirt', 2),\n",
       "  ('shopping', 2),\n",
       "  ('soccer', 2),\n",
       "  ('some', 2),\n",
       "  ('state', 1),\n",
       "  ('stomach', 1),\n",
       "  ('stormy', 1),\n",
       "  ('strip', 1),\n",
       "  ('suck', 1),\n",
       "  ('take', 1),\n",
       "  ('tank', 1),\n",
       "  ('team', 2),\n",
       "  ('that', 3),\n",
       "  ('the', 10),\n",
       "  ('their', 1),\n",
       "  ('them', 3),\n",
       "  ('then', 7),\n",
       "  ('there', 2),\n",
       "  ('these', 1),\n",
       "  ('they', 3),\n",
       "  ('third', 1),\n",
       "  ('thong', 1),\n",
       "  ('to', 3),\n",
       "  ('today', 1),\n",
       "  ('too', 1),\n",
       "  ('top', 1),\n",
       "  ('tourney', 3),\n",
       "  ('track', 1),\n",
       "  ('two', 2),\n",
       "  ('us', 1),\n",
       "  ('victoria', 1),\n",
       "  ('watch', 1),\n",
       "  ('we', 12),\n",
       "  ('weekend', 1),\n",
       "  ('which', 1),\n",
       "  ('white', 1),\n",
       "  ('who', 1),\n",
       "  ('why', 1),\n",
       "  ('wicked', 1),\n",
       "  ('win', 1),\n",
       "  ('wisconsin', 1),\n",
       "  ('with', 4),\n",
       "  ('work', 1),\n",
       "  ('your', 1)]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "# Build LDA model\n",
    "lda_model = LdaModel(corpus=corpus,\n",
    "                   id2word=id2word,\n",
    "                   num_topics=10, \n",
    "                   random_state=100,\n",
    "                   update_every=1,\n",
    "                   chunksize=100,\n",
    "                   passes=10,\n",
    "                   alpha='auto',\n",
    "                   per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.006*\"and\" + 0.005*\"i\" + 0.005*\"a\" + 0.005*\"we\" + 0.004*\"the\" + 0.004*\"then\" + 0.004*\"get\" + 0.004*\"be\" + 0.004*\"my\" + 0.004*\"of\"'), (1, '0.004*\"and\" + 0.004*\"i\" + 0.004*\"s\" + 0.004*\"m\" + 0.004*\"a\" + 0.004*\"my\" + 0.004*\"that\" + 0.004*\"be\" + 0.004*\"it\" + 0.004*\"summer\"'), (2, '0.004*\"and\" + 0.004*\"be\" + 0.004*\"i\" + 0.004*\"the\" + 0.004*\"then\" + 0.004*\"we\" + 0.004*\"a\" + 0.004*\"get\" + 0.004*\"my\" + 0.004*\"mom\"'), (3, '0.039*\"the\" + 0.026*\"i\" + 0.026*\"a\" + 0.026*\"be\" + 0.026*\"have\" + 0.026*\"game\" + 0.026*\"in\" + 0.026*\"city\" + 0.026*\"ve\" + 0.014*\"and\"'), (4, '0.004*\"and\" + 0.004*\"i\" + 0.004*\"the\" + 0.004*\"a\" + 0.004*\"get\" + 0.004*\"then\" + 0.004*\"my\" + 0.004*\"we\" + 0.004*\"with\" + 0.004*\"s\"'), (5, '0.004*\"and\" + 0.004*\"i\" + 0.004*\"a\" + 0.004*\"be\" + 0.004*\"we\" + 0.004*\"the\" + 0.004*\"get\" + 0.004*\"then\" + 0.004*\"my\" + 0.004*\"of\"'), (6, '0.075*\"and\" + 0.042*\"i\" + 0.030*\"a\" + 0.030*\"be\" + 0.027*\"the\" + 0.026*\"my\" + 0.025*\"we\" + 0.022*\"get\" + 0.017*\"of\" + 0.017*\"then\"'), (7, '0.005*\"and\" + 0.004*\"the\" + 0.004*\"we\" + 0.004*\"then\" + 0.004*\"get\" + 0.004*\"be\" + 0.004*\"my\" + 0.004*\"i\" + 0.004*\"a\" + 0.004*\"of\"'), (8, '0.004*\"i\" + 0.004*\"and\" + 0.004*\"a\" + 0.004*\"the\" + 0.004*\"have\" + 0.004*\"s\" + 0.004*\"it\" + 0.004*\"be\" + 0.004*\"game\" + 0.004*\"soccer\"'), (9, '0.096*\"i\" + 0.054*\"and\" + 0.036*\"m\" + 0.036*\"a\" + 0.026*\"summer\" + 0.020*\"school\" + 0.019*\"do\" + 0.019*\"game\" + 0.019*\"soccer\" + 0.019*\"have\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "print(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
